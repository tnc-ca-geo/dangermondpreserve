{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9704004b",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25bf4167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T18:06:23.149256Z",
     "start_time": "2023-02-15T18:06:13.958121Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import os, glob, linecache, uuid\n",
    "import xml.etree.ElementTree as ETE\n",
    "import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from scipy.stats import zscore\n",
    "import re\n",
    "import ipdb\n",
    "\n",
    "uuid_gen = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613154f3",
   "metadata": {},
   "source": [
    "# Pull Data from LEVs into CSVs for Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d4844",
   "metadata": {},
   "source": [
    "## Access LEVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da6ff46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T18:06:23.197139Z",
     "start_time": "2023-02-15T18:06:23.183140Z"
    }
   },
   "outputs": [],
   "source": [
    "#Point to folder with new levs\n",
    "data_dir = \"C:/Users/jinsu.elhance/Desktop/2023_06_16_WellSurvey/Well Data\"\n",
    "\n",
    "#Point to folder where jsons for dendra API can be stored\n",
    "dendra_dir = \"C:\\\\Users\\\\jinsu.elhance\\\\Box\\\\000. Jinsu Elhance\\\\DendraWork\\\\Data\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "115a302b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T18:06:24.505122Z",
     "start_time": "2023-02-15T18:06:23.205134Z"
    }
   },
   "outputs": [],
   "source": [
    "#Find all LEV files\n",
    "LEV_dir_list = glob.glob(f\"{data_dir}/*/*.lev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bce9f0b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T00:27:12.711066Z",
     "start_time": "2022-12-06T00:27:12.693651Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define helper fn\n",
    "def find_data_rows(lines):\n",
    "    i = 0 \n",
    "    while i < len(lines):\n",
    "        if lines[i] == \"[Data]\\r\\n\":\n",
    "            return i \n",
    "        i += 1\n",
    "        \n",
    "def find_discrete_ts(df): #Finds contiguous segments of time series data\n",
    "    df['tdelta'] = df['timestamp_utc'].diff()\n",
    "    gap_finder = df.loc[df['tdelta'] > timedelta(days=7)]\n",
    "    tseries = []\n",
    "\n",
    "    indices = np.append([0], gap_finder.index.values)\n",
    "    indices = np.append(indices, df.index.values[-1]+1)\n",
    "\n",
    "    indexr = [(indices[i-1], indices[i]-1) for i in range(1, len(indices))]\n",
    "    \n",
    "#     For each gap, section our the data\n",
    "    for _indexr in indexr:\n",
    "        tseries.append(df[_indexr[0]:_indexr[1]])\n",
    "        \n",
    "    return tseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "510cb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store all device metadata and data\n",
    "WellDevices = pd.DataFrame()\n",
    "WellData = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c7b539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse each LEV to extract data and metadata\n",
    "for lev in LEV_dir_list:\n",
    "    if \"Compensated\" in lev:\n",
    "        continue\n",
    "    with open(lev, newline=\"\\n\") as lev_text:\n",
    "        lev_lines = lev_text.readlines()\n",
    "        _dataStart = find_data_rows(lev_lines) + 2\n",
    "        \n",
    "        #Pull out metadata from datafile\n",
    "        _metadata = {}\n",
    "        for _mdx in lev_lines[10:_dataStart]:\n",
    "            _spltKeyDat = _mdx.replace(\" \", \"\").strip().split(\"=\")\n",
    "            if len(_spltKeyDat) > 1:\n",
    "                if _spltKeyDat[0] == \"Unit\" and \"LevelUnit\" in _metadata.keys():\n",
    "                    _spltKeyDat[0] = \"TemperatureUnit\"\n",
    "                elif _spltKeyDat[0] == \"Unit\":\n",
    "                    _spltKeyDat[0] = \"LevelUnit\"\n",
    "                _metadata[_spltKeyDat[0]] = _spltKeyDat[1]\n",
    "\n",
    "        #Which well?\n",
    "        location = (_metadata.get(\"Location\") or \"Unknown\").replace(\"/\", \"\").replace(\"#\", \"\").replace(\" \",\"\").lower()\n",
    "        instrumentType = _metadata.get('Instrumenttype' or \"Unknown\")\n",
    "        \n",
    "        ## Find Data Pointer\n",
    "        _df = pd.read_fwf(lev, skiprows=_dataStart, names=[\"date\", \"time\", \"level\", \"temperature\"], encoding='iso-8859-1')\n",
    "        _df = _df.iloc[:-1]\n",
    "        _df = _df.set_index(_df.agg(('{0[date]}{0[time]}' + f\"{location}{instrumentType}\").format, axis=1).apply(lambda x: hash(x)))\n",
    "        _df['date'] = pd.to_datetime(_df['date'])\n",
    "        \n",
    "        #Update Metadata\n",
    "        _metadata['Data_start_date'] = min(_df['date'])\n",
    "        _metadata['Data_end_date'] = max(_df['date'])\n",
    "        _metadata['TemperatureUnit'] = _metadata['TemperatureUnit'][-1]\n",
    "        _metadata[\"Location\"] = location\n",
    "        _metadata['Data_source'] = \"lev\"\n",
    "        \n",
    "        #Save data_units to different columns\n",
    "        _df = _df.dropna(subset='temperature')\n",
    "        if _metadata['TemperatureUnit'] == \"C\":\n",
    "            _df = _df.rename(columns = {'temperature': 'temperature_C'})\n",
    "            _df['temperature_F'] = np.NaN\n",
    "        elif _metadata['TemperatureUnit'] == \"F\":\n",
    "            _df = _df.rename(columns = {'temperature': 'temperature_F'})\n",
    "            _df['temperature_C'] = np.NaN   \n",
    "        \n",
    "        _df = _df.dropna(subset='level')\n",
    "        if _metadata['LevelUnit'] == \"ft\":\n",
    "            _df = _df.rename(columns = {'level': 'level_ft'})\n",
    "            _df['level_m'] = np.NaN\n",
    "        elif _metadata['LevelUnit'] == \"m\":\n",
    "            _df = _df.rename(columns = {'level': 'level_m'})\n",
    "            _df['level_ft'] = np.NaN \n",
    "            \n",
    "        _df['TIMESTAMP'] = pd.to_datetime(_df['date'].astype(str) + \" \" + _df['time'].astype(str))\n",
    "        _df = _df.drop(['date', 'time'], axis=1)\n",
    "        _df = _df.dropna(subset='TIMESTAMP')\n",
    "        \n",
    "        #Save data to dataframes\n",
    "        if location in WellData.keys():\n",
    "            if 'xle_lev' in WellData[location].keys():\n",
    "                WellData[location]['xle_lev'] = pd.concat([WellData[location]['xle_lev'], _df], axis=0)\n",
    "            else:\n",
    "                WellData[location]['xle_lev'] = _df\n",
    "        else:\n",
    "            WellData[location] = {}\n",
    "            WellData[location]['xle_lev'] = _df\n",
    "\n",
    "        WellDevices = pd.concat([WellDevices, pd.DataFrame(_metadata, index=[instrumentType+location])], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d1da711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for well in WellData.keys():\n",
    "    if 'xle_lev' in WellData[well]:\n",
    "        WellData[well]['xle_lev'].index.names = [\"seq_id\"]\n",
    "        WellData[well]['xle_lev'] = WellData[well]['xle_lev'].drop_duplicates(keep=\"first\")\n",
    "        WellData[well]['xle_lev'].to_csv(f\"{dendra_dir}/{well}_dendra_xle_lev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d51c42",
   "metadata": {},
   "source": [
    "# Upload data to Dendra by sending it to Scott Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ce862",
   "metadata": {},
   "source": [
    "** Only proceed when data is uploaded **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52f6b1",
   "metadata": {},
   "source": [
    "# Verify that Dendra has Stations for each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebda8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "874ba799",
   "metadata": {},
   "source": [
    "# Create Datastreams if necessary\n",
    "1. Ensure you hide xlsx datastreams or empty ones or datastreams on empty stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datastreams = [(\"level\", \"xle_lev\"), (\"temperature\", \"xle_lev\"), (\"temperature\", \"xlsx\"), (\"level\", \"xlsx\")]\n",
    "# datastream_template_paths = [\"xle_lev_level.json\", \"xlsx_level.json\", \"xle_lev_temp.json\", \"xlsx_temp.json\"]\n",
    "# datastream_templates = []\n",
    "\n",
    "# for t in datastream_template_paths: \n",
    "#     with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/{t}\") as template_raw:\n",
    "#         datastream_templates.append(deepcopy(json.load(template_raw)))\n",
    "        \n",
    "# #get station names and slugs\n",
    "# datastreams_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/*.json\")\n",
    "# station_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Stations/64*.json\")\n",
    "\n",
    "# for station_path in station_glob:\n",
    "#     with open(station_path, \"r\") as station_file:\n",
    "#         station_data = json.load(station_file)\n",
    "#         station_name = station_data['slug'].lower().replace(\"dangermond-\", \"\")\n",
    "#         for i in range(4):\n",
    "#             datastream = datastreams[i]\n",
    "#             if os.path.exists(f\"C:/Users/jinsu.elhance/Box/Wells/WellsDatasheets/HistoricalWellSynthesis/Data/Dendra_Uploads/{station_name}_dendra_{datastream[1]}.csv\"):\n",
    "# #                 print(station_name, datastream)\n",
    "#                 datastream_template = deepcopy(datastream_templates[i])\n",
    "#                 datastream_template['datapoints_config'][0][\"params\"][\"query\"][\"fc\"] = datastream_template['datapoints_config'][0][\"params\"][\"query\"][\"fc\"].replace(\"WELL\", station_name)\n",
    "#                 datastream_template['description'] = datastream_template['description'].replace(\"STATIONNAME\", station_data['full_name'].replace(\"Dangermond \", \"\"))\n",
    "#                 datastream_template['datapoints_config_refd'][0]['params']['query']['fc'] = datastream_template['datapoints_config'][0][\"params\"][\"query\"][\"fc\"]\n",
    "#                 datastream_template['station_id'] = station_data['_id']\n",
    "    #              Write the JSON objects to a file\n",
    "#                 with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/{station_name}.{datastream[0]}.{datastream[1]}.datastream.json\", 'w') as json_file:\n",
    "#                     json.dump(datastream_template, json_file, indent=4)\n",
    "\n",
    "#Hide datastreams on hidden stations\n",
    "# hidden_stations = []\n",
    "\n",
    "# station_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Stations/64*.json\")\n",
    "\n",
    "# for station_path in station_glob:\n",
    "#     with open(station_path, \"r\") as station_file:\n",
    "#         station_data = json.load(station_file)\n",
    "#         if station_data['is_hidden'] == True:\n",
    "#             hidden_stations.append(station_data[\"_id\"])\n",
    "            \n",
    "# datastreams_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/64*.json\")\n",
    "\n",
    "# for datastream_path in datastreams_glob:\n",
    "#     with open(datastream_path, \"r\") as datastream_file:\n",
    "#         datastream_data = json.load(datastream_file)\n",
    "#         if datastream_data['station_id'] in hidden_stations:\n",
    "#             datastream_data['is_hidden'] = True\n",
    "#     #              Write the JSON objects to a file\n",
    "#             with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/patch.{datastream_data['_id']}.datastream.json\", 'w') as json_file:\n",
    "#                 json.dump(datastream_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1dda88",
   "metadata": {},
   "source": [
    "# Annotate Survey Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc3b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using well data uploaded to dendra, find starts and ends of data gaps to create annotations with\n",
    "\n",
    "#The code below writes gap_markers\n",
    "# well_data_list = []\n",
    "\n",
    "# for data_f in dendra_data_glob:\n",
    "#     well = os.path.basename(data_f).replace(\"_dendra\", \"\").replace(\"_xle_lev.csv\", \"\").replace(\"_xlsx.csv\", \"\")\n",
    "#     well_data = pd.read_csv(data_f)\n",
    "#     well_data['well'] = well\n",
    "#     well_data['TIMESTAMP'] = pd.to_datetime(well_data['TIMESTAMP'])\n",
    "#     well_data['tdelta'] = well_data['TIMESTAMP'].diff()\n",
    "#     well_data_list.append(well_data)\n",
    "    \n",
    "# well_data_full = pd.concat(well_data_list)\n",
    "# well_data_full.head()\n",
    "\n",
    "# gap_finder = well_data_full.loc[well_data_full['tdelta'] > timedelta(days=1)]\n",
    "# gap_finder['gap_start'] = gap_finder['TIMESTAMP'] - gap_finder['tdelta']\n",
    "# gap_ends = gap_finder.groupby(gap_finder['TIMESTAMP'].map(lambda x: (x.year, x.month, x.day)))['well'].agg(lambda x: set(x))\n",
    "# gap_starts = gap_finder.groupby(gap_finder['gap_start'].map(lambda x: (x.year, x.month, x.day)))['well'].agg(lambda x: set(x))\n",
    "\n",
    "# with open(f\"C:\\\\Users\\\\jinsu.elhance\\\\Box\\\\000. Jinsu Elhance\\\\DendraWork\\\\Annotations\\\\template.annotation.json\", encoding=\"utf-8\") as anno_template_file:\n",
    "    \n",
    "#     anno_template_json = json.load(anno_template_file)\n",
    "#     anno_template_json['station_ids'] = []\n",
    "#     anno_template_json['intervals'] = []\n",
    "    \n",
    "#     for i, anno in gap_markers.iterrows():\n",
    "#         anno_template = deepcopy(anno_template_json)\n",
    "#         wells = anno['wells'].split(\"-\")\n",
    "#         anno_template['intervals'].append({\n",
    "#             \"begins_at\":f\"{anno['BEGIN_DATE']}T00:00:00.000Z\",\n",
    "#             \"ends_before\":f\"{anno['END_DATE']}T23:59:00.000Z\",\n",
    "#         })\n",
    "#         anno_template['title'] = anno_template['title'].replace(\"START\", str(anno['BEGIN_DATE'])).replace(\"END\", str(anno['END_DATE']))\n",
    "#         anno_template['station_ids'] = [stations_dict[well] for well in wells]\n",
    "        \n",
    "#       Write the JSON objects to a file\n",
    "#         with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Annotations/{anno['BEGIN_DATE']}.annotation.json\", 'w') as json_file:\n",
    "#             json.dump(anno_template, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b54cdc",
   "metadata": {},
   "source": [
    "# Identify and Annotate Outliers on Datastreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # datastream_df_dict = {}\n",
    "\n",
    "# #Create empty anno json\n",
    "# with open(f\"C:\\\\Users\\\\jinsu.elhance\\\\Box\\\\000. Jinsu Elhance\\\\DendraWork\\\\Annotations\\\\outlier_template.annotation.json\", encoding=\"utf-8\") as anno_template_file:\n",
    "    \n",
    "#     anno_template_json = json.load(anno_template_file)\n",
    "    \n",
    "# anno_template_json = deepcopy(anno_template_json)\n",
    "# anno_template_json['datastream_ids'] = []\n",
    "\n",
    "# #Interval looks like:\n",
    "# \"\"\"\n",
    "# 'intervals':[\n",
    "# {'begins_at' : '2010-02-23T19:30:00.000',\n",
    "#  'ends_before' : '2010-02-23T19:30:00.000'},\n",
    "#  ...\n",
    "# ]\n",
    "# \"\"\"\n",
    "\n",
    "# #Query Dendra API to fetch Datapoints for each stream (from Station IDs)\n",
    "# for station_name in stations_dict:\n",
    "#     station_id = stations_dict[station_name]\n",
    "#     station_datastreams = dendra.list_datastreams_by_station_id(station_id)\n",
    "    \n",
    "#     for datastream in station_datastreams: \n",
    "#         datastream_id = datastream['_id']\n",
    "#         dendra_fetch = dendra.get_datapoints(datastream_id, begins_at=\"2000-01-01T00:00:00\", time_type=\"local\").reset_index()\n",
    "#         dendra_fetch = dendra_fetch.rename(columns={dendra_fetch.columns[2]:\"v\"})\n",
    "        \n",
    "#         if dendra_fetch.shape[0] == 0:\n",
    "#             continue\n",
    "            \n",
    "#         outlier_anno = deepcopy(anno_template_json)\n",
    "#         outlier_indices = []\n",
    "        \n",
    "#         #Iterate over any contiguous sections of timeseries data and find outliers.\n",
    "#         for ts in find_discrete_ts(dendra_fetch):\n",
    "#             clf = IsolationForest(random_state=0, contamination=0.0005).fit(ts[['v']])\n",
    "#             ts.loc[:, 'outlier'] = clf.predict(ts[['v']])\n",
    "#             outlier_indices = np.append(outlier_indices, ts.loc[ts['outlier'] == -1].index)\n",
    "        \n",
    "#         #Add indices where values are non-positive\n",
    "#         outlier_indices = np.append(outlier_indices, list(dendra_fetch.loc[dendra_fetch.v <= 0].index.values)).astype(int)\n",
    "        \n",
    "#         if len(outlier_indices) == 0:\n",
    "#             continue\n",
    "        \n",
    "#         #Create outlier anno timestamp objects\n",
    "#         time_stamps = [{\n",
    "#             'begins_at': str(dendra_fetch.iloc[index]['timestamp_utc']).replace(\" \",\"T\").replace(\"+00:00\",\".000Z\"),\n",
    "#             'ends_before': str(dendra_fetch.iloc[index]['timestamp_utc']).replace(\" \",\"T\").replace(\"+00:00\",\".000Z\")\n",
    "#         } for index in outlier_indices]\n",
    "        \n",
    "#         #Create annotation object\n",
    "#         outlier_anno['datastream_ids'] = [datastream_id]\n",
    "#         outlier_anno['intervals'] = time_stamps\n",
    "#         outlier_anno['station_ids'] = [station_id]\n",
    "#         outlier_anno['title'] = f\"Outlier Filter for {station_id}:{datastream['name']}\"\n",
    "#         outlier_anno['description'] = \"Isolation Forest Outlier Filter + Removing Negative Points\"\n",
    "\n",
    "#         print(f\"{station_name}:{datastream['name']}:{len(time_stamps)} outliers found\")\n",
    "# #         Write the JSON objects to a file\n",
    "#         with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Annotations/{datastream_id}.outliers.annotation.json\", 'w') as json_file:\n",
    "#             json.dump(outlier_anno, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab271ab",
   "metadata": {},
   "source": [
    "# Create Derived Datastreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Iterate through stations\n",
    "#     #Find level datastreams\n",
    "#     #Find temperature datastreams\n",
    "   \n",
    "# with open(f\"C:\\\\Users\\\\jinsu.elhance\\\\Box\\\\000. Jinsu Elhance\\\\DendraWork\\\\Datastreams\\\\Derived\\\\level.template.json\", encoding=\"utf-8\") as level_template_file:\n",
    "    \n",
    "#     level_template_json = json.load(level_template_file)\n",
    "    \n",
    "# level_template_json = deepcopy(level_template_json)\n",
    "\n",
    "# with open(f\"C:\\\\Users\\\\jinsu.elhance\\\\Box\\\\000. Jinsu Elhance\\\\DendraWork\\\\Datastreams\\\\Derived\\\\temp.template.json\", encoding=\"utf-8\") as temp_template_file:\n",
    "    \n",
    "#     temp_template_json = json.load(temp_template_file)\n",
    "    \n",
    "# temp_template_json = deepcopy(temp_template_json)\n",
    "\n",
    "# for station_name in stations_dict:\n",
    "#     station_id = stations_dict[station_name]\n",
    "#     station_datastreams = dendra.list_datastreams_by_station_id(station_id)\n",
    "#     station_name_C = dendra.get_meta_station_by_id(station_id)['full_name']\n",
    "        \n",
    "#     level_xle, level_xlsx, temp_xle, temp_xlsx = None, None, None, None\n",
    "#     level_json = deepcopy(level_template_json)\n",
    "#     temp_json = deepcopy(temp_template_json)\n",
    "#     to_hide = True\n",
    "    \n",
    "#     for datastream in station_datastreams: \n",
    "#         datastream_id = datastream['_id']\n",
    "        \n",
    "#         if datastream['name'] == \"Well Water Level xle/lev\":\n",
    "#             level_xle = datastream\n",
    "#         elif datastream['name'] == \"Well Water Level xlsx\":\n",
    "#             level_xlsx = datastream\n",
    "#         elif datastream['name'] == \"Well Water Temperature xle/lev\":\n",
    "#             temp_xle = datastream\n",
    "#         elif datastream['name'] == \"Well Water Temperature xlsx\":\n",
    "#             temp_xlsx = datastream\n",
    "\n",
    "#     #Create Derived Level datastream\n",
    "#     level_json['description'] = f\"Derived datastream for {station_name_C} Well Water Level\"\n",
    "#     level_json['derived_from_datastream_ids'] = [level_xle['_id'], level_xlsx['_id']]\n",
    "#     level_json['name'] = \"Well Water Level\"\n",
    "#     level_json['station_id'] = station_id\n",
    "#     level_json['is_hidden'] = dendra.get_meta_datastream_by_id(level_xle['_id'])['is_hidden'] and dendra.get_meta_datastream_by_id(level_xlsx['_id'])['is_hidden']\n",
    "    \n",
    "#     with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Derived/{station_name}.level.derived.json\", 'w') as json_file:\n",
    "#         json.dump(level_json, json_file, indent=4)\n",
    "    \n",
    "#     #Create Derived temperature datastream\n",
    "#     temp_json['description'] = f\"Derived datastream for {station_name_C} Well Water Temperature\"\n",
    "#     temp_json['derived_from_datastream_ids'] = [temp_xle['_id'], temp_xlsx['_id']]\n",
    "#     temp_json['name'] = \"Well Water Temperature\"\n",
    "#     temp_json['station_id'] = station_id\n",
    "#     temp_json['is_hidden'] = dendra.get_meta_datastream_by_id(temp_xle['_id'])['is_hidden'] and dendra.get_meta_datastream_by_id(temp_xlsx['_id'])['is_hidden']\n",
    "    \n",
    "#     with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Derived/{station_name}.temp.derived.json\", 'w') as json_file:\n",
    "#         json.dump(temp_json, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb560fc7",
   "metadata": {},
   "source": [
    "# Barometrically Correct (if possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb813",
   "metadata": {},
   "source": [
    "# Use Survey123 Data to Create Station Attributes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
