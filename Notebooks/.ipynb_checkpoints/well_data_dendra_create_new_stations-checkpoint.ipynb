{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3edde4",
   "metadata": {},
   "source": [
    "# Upload Well Data to Dendra Automatically\n",
    "\n",
    "Hello, welcome to this script. In general, this script is a tool for taking XLE/LEV data files from Solinst Loggers and ingesting them into the Dendra platform. The script was written primarily for use at the Jack and Laura Dangermond Preserve, but can be modified openly for any organization. The script makes heavy use of a python library written by the Dendra team called dendra_api_client. Which can be found here:\n",
    "\n",
    "[Dendra API Client Python Library](https://github.com/DendraScience/dendra-api-client-python/tree/master)\n",
    "\n",
    "In order to properly use this tool, you'll need a Dendra account associated with the TNC organization. Go ahead and create one if you haven't already. \n",
    "\n",
    "On Dendra, each groundwater well is its own station, and each holds several datastreams related to water level, water temperature, and sometimes barometric pressure (if there is a barometer nearby). \n",
    "\n",
    "This tool takes XLE/LEV files, reads the metadata from them, and then documents the data correctly on Dendra using the API. The steps taken are the following:\n",
    "\n",
    "1. Read metadata from the XLE/LEVs into CSVs for Upload\n",
    "2. Request that the user send an email to Scott Smith with the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62e4362",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71659b97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T18:06:23.149256Z",
     "start_time": "2023-02-15T18:06:13.958121Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import os, glob, linecache, uuid\n",
    "import xml.etree.ElementTree as ETE\n",
    "import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from scipy.stats import zscore\n",
    "import re\n",
    "import ipdb\n",
    "\n",
    "uuid_gen = uuid.uuid4()\n",
    "\n",
    "den_api_lib_path = \"C:\\\\Users\\\\jinsu.elhance\\\\Box\\\\000. Jinsu Elhance\\\\Github\\\\dendra-api-client-python\"\n",
    "sys.path.append(den_api_lib_path)\n",
    "import dendra_api_client as dendra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d32e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authenticate Dendra API\n",
    "# If you have a login and the data is not public, you must authenticatte using your Dendra login\n",
    "dendra.authenticate('jinsu.elhance@tnc.org')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d767d88",
   "metadata": {},
   "source": [
    "# Pull Data from LEVs into CSVs for Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23f77d",
   "metadata": {},
   "source": [
    "## Access LEVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac31cd7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T18:06:23.197139Z",
     "start_time": "2023-02-15T18:06:23.183140Z"
    }
   },
   "outputs": [],
   "source": [
    "#Point to folder with new levs\n",
    "data_dir = \"C:/Users/jinsu.elhance/Desktop/2023_06_16_WellSurvey/Well Data\"\n",
    "\n",
    "#Point to folder where jsons for dendra API can be stored\n",
    "dendra_dir = \"C:\\\\Users\\\\jinsu.elhance\\\\Box\\\\000. Jinsu Elhance\\\\DendraWork\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6313c1b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T18:06:24.505122Z",
     "start_time": "2023-02-15T18:06:23.205134Z"
    }
   },
   "outputs": [],
   "source": [
    "#Find all LEV files\n",
    "LEV_dir_list = glob.glob(f\"{data_dir}/*/*.lev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c23c7cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T00:27:12.711066Z",
     "start_time": "2022-12-06T00:27:12.693651Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define helper fn\n",
    "def find_data_rows(lines):\n",
    "    i = 0 \n",
    "    while i < len(lines):\n",
    "        if lines[i] == \"[Data]\\r\\n\":\n",
    "            return i \n",
    "        i += 1\n",
    "        \n",
    "def find_discrete_ts(df): #Finds contiguous segments of time series data\n",
    "    df['tdelta'] = df['timestamp_utc'].diff()\n",
    "    gap_finder = df.loc[df['tdelta'] > timedelta(days=7)]\n",
    "    tseries = []\n",
    "\n",
    "    indices = np.append([0], gap_finder.index.values)\n",
    "    indices = np.append(indices, df.index.values[-1]+1)\n",
    "\n",
    "    indexr = [(indices[i-1], indices[i]-1) for i in range(1, len(indices))]\n",
    "    \n",
    "#     For each gap, section our the data\n",
    "    for _indexr in indexr:\n",
    "        tseries.append(df[_indexr[0]:_indexr[1]])\n",
    "        \n",
    "    return tseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e89e263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store all device metadata and data\n",
    "WellDevices = pd.DataFrame()\n",
    "WellData = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc3648ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse each LEV to extract data and metadata\n",
    "for lev in LEV_dir_list:\n",
    "    if \"Compensated\" in lev:\n",
    "        continue\n",
    "    with open(lev, newline=\"\\n\") as lev_text:\n",
    "        lev_lines = lev_text.readlines()\n",
    "        _dataStart = find_data_rows(lev_lines) + 2\n",
    "        \n",
    "        #Pull out metadata from datafile\n",
    "        _metadata = {}\n",
    "        for _mdx in lev_lines[10:_dataStart]:\n",
    "            _spltKeyDat = _mdx.replace(\" \", \"\").strip().split(\"=\")\n",
    "            if len(_spltKeyDat) > 1:\n",
    "                if _spltKeyDat[0] == \"Unit\" and \"LevelUnit\" in _metadata.keys():\n",
    "                    _spltKeyDat[0] = \"TemperatureUnit\"\n",
    "                elif _spltKeyDat[0] == \"Unit\":\n",
    "                    _spltKeyDat[0] = \"LevelUnit\"\n",
    "                _metadata[_spltKeyDat[0]] = _spltKeyDat[1]\n",
    "\n",
    "        #Which well?\n",
    "        location = (_metadata.get(\"Location\") or \"Unknown\").replace(\"/\", \"\").replace(\"#\", \"\").replace(\" \",\"\").lower()\n",
    "        instrumentType = _metadata.get('Instrumenttype' or \"Unknown\")\n",
    "        \n",
    "        ## Find Data Pointer\n",
    "        _df = pd.read_fwf(lev, skiprows=_dataStart, names=[\"date\", \"time\", \"level\", \"temperature\"], encoding='iso-8859-1')\n",
    "        _df = _df.iloc[:-1]\n",
    "        _df = _df.set_index(_df.agg(('{0[date]}{0[time]}' + f\"{location}{instrumentType}\").format, axis=1).apply(lambda x: hash(x)))\n",
    "        _df['date'] = pd.to_datetime(_df['date'])\n",
    "        \n",
    "        #Update Metadata\n",
    "        _metadata['Data_start_date'] = min(_df['date'])\n",
    "        _metadata['Data_end_date'] = max(_df['date'])\n",
    "        _metadata['TemperatureUnit'] = _metadata['TemperatureUnit'][-1]\n",
    "        _metadata[\"Location\"] = location\n",
    "        _metadata['Data_source'] = \"lev\"\n",
    "        \n",
    "        #Save data_units to different columns\n",
    "        _df = _df.dropna(subset='temperature')\n",
    "        if _metadata['TemperatureUnit'] == \"C\":\n",
    "            _df = _df.rename(columns = {'temperature': 'temperature_C'})\n",
    "            _df['temperature_F'] = np.NaN\n",
    "        elif _metadata['TemperatureUnit'] == \"F\":\n",
    "            _df = _df.rename(columns = {'temperature': 'temperature_F'})\n",
    "            _df['temperature_C'] = np.NaN   \n",
    "        \n",
    "        _df = _df.dropna(subset='level')\n",
    "        if _metadata['LevelUnit'] == \"ft\":\n",
    "            _df = _df.rename(columns = {'level': 'level_ft'})\n",
    "            _df['level_m'] = np.NaN\n",
    "        elif _metadata['LevelUnit'] == \"m\":\n",
    "            _df = _df.rename(columns = {'level': 'level_m'})\n",
    "            _df['level_ft'] = np.NaN \n",
    "            \n",
    "        _df['TIMESTAMP'] = pd.to_datetime(_df['date'].astype(str) + \" \" + _df['time'].astype(str))\n",
    "        _df = _df.drop(['date', 'time'], axis=1)\n",
    "        _df = _df.dropna(subset='TIMESTAMP')\n",
    "        \n",
    "        #Save data to dataframes\n",
    "        if location in WellData.keys():\n",
    "            if 'xle_lev' in WellData[location].keys():\n",
    "                WellData[location]['xle_lev'] = pd.concat([WellData[location]['xle_lev'], _df], axis=0)\n",
    "            else:\n",
    "                WellData[location]['xle_lev'] = _df\n",
    "        else:\n",
    "            WellData[location] = {}\n",
    "            WellData[location]['xle_lev'] = _df\n",
    "\n",
    "        WellDevices = pd.concat([WellDevices, pd.DataFrame(_metadata, index=[instrumentType+location])], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63ffc6e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for well in WellData.keys():\n",
    "    if 'xle_lev' in WellData[well]:\n",
    "        WellData[well]['xle_lev'].index.names = [\"seq_id\"]\n",
    "        WellData[well]['xle_lev'] = WellData[well]['xle_lev'].drop_duplicates(keep=\"first\")\n",
    "        WellData[well]['xle_lev'].to_csv(f\"{dendra_dir}/Data/{well}_dendra_xle_lev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc09f7d",
   "metadata": {},
   "source": [
    "# Upload data to Dendra by sending it to Scott Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c70ff",
   "metadata": {},
   "source": [
    "** Only proceed when data is uploaded **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17615cbf",
   "metadata": {},
   "source": [
    "# Verify that Dendra has Stations for each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = dendra.list_stations(orgslug='tnc',query_add={\"description\":\"Groundwater well @ Dangermond Preserve\"})\n",
    "station_metas = [dendra.get_meta_station_by_id(station['_id']) for station in stations] \n",
    "\n",
    "for well in WellData.keys():\n",
    "    if well in [well_station['slug'].replace(\"dangermond-\") for well_station in station_metas]:\n",
    "        print(f\"Found {well} station on Dendra.\")\n",
    "    else:\n",
    "        print(f\"Double-check that the well name in WellData matches the 'slug' of a Dendra Well Station. If you need to create a station, run the following cell\")\n",
    "        \n",
    "#Proceed if all wells are matched on dendra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7255d4",
   "metadata": {},
   "source": [
    "# Create Station and Datastreams\n",
    "If Dendra doesn't have a station to host your data yet, run this cell. It will create a station with your provided name. The station will have 5 datastreams which can be empty if there is no data to fill them. The 5 datastreams are:\n",
    "- Level from XLE/LEV (1 stream) and XLSX (1 stream) files\n",
    "- Temperature from XLE/LEV (1 stream) and XLSX (1 stream) files\n",
    "- Depth to Groundwater (1 stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2aa985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First create level and temperature datastreams\n",
    "datastreams = [(\"level\", \"xle_lev\"), (\"temperature\", \"xle_lev\"), (\"temperature\", \"xlsx\"), (\"level\", \"xlsx\")]\n",
    "datastream_template_paths = [\"xle_lev_level.json\", \"xlsx_level.json\", \"xle_lev_temp.json\", \"xlsx_temp.json\"]\n",
    "datastream_templates = []\n",
    "\n",
    "for t in datastream_template_paths: \n",
    "    with open(f\"{dendra_dir}/Datastreams/{t}\") as template_raw:\n",
    "        datastream_templates.append(deepcopy(json.load(template_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611babf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the names of the stations you'd like to create. Comma separated, capitalized. Eg. 'Tinta 11B','Lower Jalama Vaqueros'\n",
      "Tinta 11B\n",
      "Will create stations ['Tinta 11B']\n"
     ]
    }
   ],
   "source": [
    "stations_to_create = input(\"Enter the names of the stations you'd like to create. Comma separated, capitalized. Eg. 'Tinta 11B','Lower Jalama Vaqueros'\\n\")\n",
    "stations_to_create = stations_to_create.split(\",\")\n",
    "print(f\"Will create stations {stations_to_create}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get station names and slugs\n",
    "# datastreams_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/*.json\")\n",
    "# station_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Stations/64*.json\")\n",
    "\n",
    "# for station_path in station_glob:\n",
    "#     with open(station_path, \"r\") as station_file:\n",
    "#         station_data = json.load(station_file)\n",
    "#         station_name = station_data['slug'].lower().replace(\"dangermond-\", \"\")\n",
    "#         for i in range(4):\n",
    "#             datastream = datastreams[i]\n",
    "#             if os.path.exists(f\"C:/Users/jinsu.elhance/Box/Wells/WellsDatasheets/HistoricalWellSynthesis/Data/Dendra_Uploads/{station_name}_dendra_{datastream[1]}.csv\"):\n",
    "# #                 print(station_name, datastream)\n",
    "#                 datastream_template = deepcopy(datastream_templates[i])\n",
    "#                 datastream_template['datapoints_config'][0][\"params\"][\"query\"][\"fc\"] = datastream_template['datapoints_config'][0][\"params\"][\"query\"][\"fc\"].replace(\"WELL\", station_name)\n",
    "#                 datastream_template['description'] = datastream_template['description'].replace(\"STATIONNAME\", station_data['full_name'].replace(\"Dangermond \", \"\"))\n",
    "#                 datastream_template['datapoints_config_refd'][0]['params']['query']['fc'] = datastream_template['datapoints_config'][0][\"params\"][\"query\"][\"fc\"]\n",
    "#                 datastream_template['station_id'] = station_data['_id']\n",
    "    #              Write the JSON objects to a file\n",
    "#                 with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/{station_name}.{datastream[0]}.{datastream[1]}.datastream.json\", 'w') as json_file:\n",
    "#                     json.dump(datastream_template, json_file, indent=4)\n",
    "\n",
    "#Hide datastreams on hidden stations\n",
    "# hidden_stations = []\n",
    "\n",
    "# station_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Stations/64*.json\")\n",
    "\n",
    "# for station_path in station_glob:\n",
    "#     with open(station_path, \"r\") as station_file:\n",
    "#         station_data = json.load(station_file)\n",
    "#         if station_data['is_hidden'] == True:\n",
    "#             hidden_stations.append(station_data[\"_id\"])\n",
    "            \n",
    "# datastreams_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/64*.json\")\n",
    "\n",
    "# for datastream_path in datastreams_glob:\n",
    "#     with open(datastream_path, \"r\") as datastream_file:\n",
    "#         datastream_data = json.load(datastream_file)\n",
    "#         if datastream_data['station_id'] in hidden_stations:\n",
    "#             datastream_data['is_hidden'] = True\n",
    "#     #              Write the JSON objects to a file\n",
    "#             with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/patch.{datastream_data['_id']}.datastream.json\", 'w') as json_file:\n",
    "#                 json.dump(datastream_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754390f2",
   "metadata": {},
   "source": [
    "1. Separate pathways for barometers and pressure transducers\n",
    "2. Create stations if necessary with location drawn from xle/lev\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efba771",
   "metadata": {},
   "source": [
    "# Create Datastreams if necessary\n",
    "1. Ensure you hide xlsx datastreams or empty ones or datastreams on empty stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192258ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the 4 datastreams created for every station.\n",
    "datastreams = [(\"level\", \"xle_lev\"), (\"temperature\", \"xle_lev\"), (\"temperature\", \"xlsx\"), (\"level\", \"xlsx\")]\n",
    "datastream_template_paths = [\"xle_lev_level.json\", \"xlsx_level.json\", \"xle_lev_temp.json\", \"xlsx_temp.json\"]\n",
    "datastream_templates = []\n",
    "\n",
    "# for t in datastream_template_paths: \n",
    "#     with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/{t}\") as template_raw:\n",
    "#         datastream_templates.append(deepcopy(json.load(template_raw)))\n",
    "        \n",
    "# #get station names and slugs\n",
    "# datastreams_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/*.json\")\n",
    "# station_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Stations/64*.json\")\n",
    "\n",
    "# for station_path in station_glob:\n",
    "#     with open(station_path, \"r\") as station_file:\n",
    "#         station_data = json.load(station_file)\n",
    "#         station_name = station_data['slug'].lower().replace(\"dangermond-\", \"\")\n",
    "#         for i in range(4):\n",
    "#             datastream = datastreams[i]\n",
    "#             if os.path.exists(f\"C:/Users/jinsu.elhance/Box/Wells/WellsDatasheets/HistoricalWellSynthesis/Data/Dendra_Uploads/{station_name}_dendra_{datastream[1]}.csv\"):\n",
    "# #                 print(station_name, datastream)\n",
    "#                 datastream_template = deepcopy(datastream_templates[i])\n",
    "#                 datastream_template['datapoints_config'][0][\"params\"][\"query\"][\"fc\"] = datastream_template['datapoints_config'][0][\"params\"][\"query\"][\"fc\"].replace(\"WELL\", station_name)\n",
    "#                 datastream_template['description'] = datastream_template['description'].replace(\"STATIONNAME\", station_data['full_name'].replace(\"Dangermond \", \"\"))\n",
    "#                 datastream_template['datapoints_config_refd'][0]['params']['query']['fc'] = datastream_template['datapoints_config'][0][\"params\"][\"query\"][\"fc\"]\n",
    "#                 datastream_template['station_id'] = station_data['_id']\n",
    "    #              Write the JSON objects to a file\n",
    "#                 with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/{station_name}.{datastream[0]}.{datastream[1]}.datastream.json\", 'w') as json_file:\n",
    "#                     json.dump(datastream_template, json_file, indent=4)\n",
    "\n",
    "#Hide datastreams on hidden stations\n",
    "# hidden_stations = []\n",
    "\n",
    "# station_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Stations/64*.json\")\n",
    "\n",
    "# for station_path in station_glob:\n",
    "#     with open(station_path, \"r\") as station_file:\n",
    "#         station_data = json.load(station_file)\n",
    "#         if station_data['is_hidden'] == True:\n",
    "#             hidden_stations.append(station_data[\"_id\"])\n",
    "            \n",
    "# datastreams_glob = glob.glob(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/64*.json\")\n",
    "\n",
    "# for datastream_path in datastreams_glob:\n",
    "#     with open(datastream_path, \"r\") as datastream_file:\n",
    "#         datastream_data = json.load(datastream_file)\n",
    "#         if datastream_data['station_id'] in hidden_stations:\n",
    "#             datastream_data['is_hidden'] = True\n",
    "#     #              Write the JSON objects to a file\n",
    "#             with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Requests/patch.{datastream_data['_id']}.datastream.json\", 'w') as json_file:\n",
    "#                 json.dump(datastream_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615f7c9",
   "metadata": {},
   "source": [
    "# Annotate Survey Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ef4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using well data uploaded to dendra, find starts and ends of data gaps to create annotations with\n",
    "\n",
    "#The code below writes gap_markers\n",
    "# well_data_list = []\n",
    "\n",
    "# for data_f in dendra_data_glob:\n",
    "#     well = os.path.basename(data_f).replace(\"_dendra\", \"\").replace(\"_xle_lev.csv\", \"\").replace(\"_xlsx.csv\", \"\")\n",
    "#     well_data = pd.read_csv(data_f)\n",
    "#     well_data['well'] = well\n",
    "#     well_data['TIMESTAMP'] = pd.to_datetime(well_data['TIMESTAMP'])\n",
    "#     well_data['tdelta'] = well_data['TIMESTAMP'].diff()\n",
    "#     well_data_list.append(well_data)\n",
    "    \n",
    "# well_data_full = pd.concat(well_data_list)\n",
    "# well_data_full.head()\n",
    "\n",
    "# gap_finder = well_data_full.loc[well_data_full['tdelta'] > timedelta(days=1)]\n",
    "# gap_finder['gap_start'] = gap_finder['TIMESTAMP'] - gap_finder['tdelta']\n",
    "# gap_ends = gap_finder.groupby(gap_finder['TIMESTAMP'].map(lambda x: (x.year, x.month, x.day)))['well'].agg(lambda x: set(x))\n",
    "# gap_starts = gap_finder.groupby(gap_finder['gap_start'].map(lambda x: (x.year, x.month, x.day)))['well'].agg(lambda x: set(x))\n",
    "\n",
    "# with open(f\"C:\\\\Users\\\\jinsu.elhance\\\\Box\\\\000. Jinsu Elhance\\\\DendraWork\\\\Annotations\\\\template.annotation.json\", encoding=\"utf-8\") as anno_template_file:\n",
    "    \n",
    "#     anno_template_json = json.load(anno_template_file)\n",
    "#     anno_template_json['station_ids'] = []\n",
    "#     anno_template_json['intervals'] = []\n",
    "    \n",
    "#     for i, anno in gap_markers.iterrows():\n",
    "#         anno_template = deepcopy(anno_template_json)\n",
    "#         wells = anno['wells'].split(\"-\")\n",
    "#         anno_template['intervals'].append({\n",
    "#             \"begins_at\":f\"{anno['BEGIN_DATE']}T00:00:00.000Z\",\n",
    "#             \"ends_before\":f\"{anno['END_DATE']}T23:59:00.000Z\",\n",
    "#         })\n",
    "#         anno_template['title'] = anno_template['title'].replace(\"START\", str(anno['BEGIN_DATE'])).replace(\"END\", str(anno['END_DATE']))\n",
    "#         anno_template['station_ids'] = [stations_dict[well] for well in wells]\n",
    "        \n",
    "#       Write the JSON objects to a file\n",
    "#         with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Annotations/{anno['BEGIN_DATE']}.annotation.json\", 'w') as json_file:\n",
    "#             json.dump(anno_template, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020eb2cf",
   "metadata": {},
   "source": [
    "# Identify and Annotate Outliers on Datastreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # datastream_df_dict = {}\n",
    "\n",
    "# #Create empty anno json\n",
    "# with open(f\"C:\\\\Users\\\\jinsu.elhance\\\\Box\\\\000. Jinsu Elhance\\\\DendraWork\\\\Annotations\\\\outlier_template.annotation.json\", encoding=\"utf-8\") as anno_template_file:\n",
    "    \n",
    "#     anno_template_json = json.load(anno_template_file)\n",
    "    \n",
    "# anno_template_json = deepcopy(anno_template_json)\n",
    "# anno_template_json['datastream_ids'] = []\n",
    "\n",
    "# #Interval looks like:\n",
    "# \"\"\"\n",
    "# 'intervals':[\n",
    "# {'begins_at' : '2010-02-23T19:30:00.000',\n",
    "#  'ends_before' : '2010-02-23T19:30:00.000'},\n",
    "#  ...\n",
    "# ]\n",
    "# \"\"\"\n",
    "\n",
    "# #Query Dendra API to fetch Datapoints for each stream (from Station IDs)\n",
    "# for station_name in stations_dict:\n",
    "#     station_id = stations_dict[station_name]\n",
    "#     station_datastreams = dendra.list_datastreams_by_station_id(station_id)\n",
    "    \n",
    "#     for datastream in station_datastreams: \n",
    "#         datastream_id = datastream['_id']\n",
    "#         dendra_fetch = dendra.get_datapoints(datastream_id, begins_at=\"2000-01-01T00:00:00\", time_type=\"local\").reset_index()\n",
    "#         dendra_fetch = dendra_fetch.rename(columns={dendra_fetch.columns[2]:\"v\"})\n",
    "        \n",
    "#         if dendra_fetch.shape[0] == 0:\n",
    "#             continue\n",
    "            \n",
    "#         outlier_anno = deepcopy(anno_template_json)\n",
    "#         outlier_indices = []\n",
    "        \n",
    "#         #Iterate over any contiguous sections of timeseries data and find outliers.\n",
    "#         for ts in find_discrete_ts(dendra_fetch):\n",
    "#             clf = IsolationForest(random_state=0, contamination=0.0005).fit(ts[['v']])\n",
    "#             ts.loc[:, 'outlier'] = clf.predict(ts[['v']])\n",
    "#             outlier_indices = np.append(outlier_indices, ts.loc[ts['outlier'] == -1].index)\n",
    "        \n",
    "#         #Add indices where values are non-positive\n",
    "#         outlier_indices = np.append(outlier_indices, list(dendra_fetch.loc[dendra_fetch.v <= 0].index.values)).astype(int)\n",
    "        \n",
    "#         if len(outlier_indices) == 0:\n",
    "#             continue\n",
    "        \n",
    "#         #Create outlier anno timestamp objects\n",
    "#         time_stamps = [{\n",
    "#             'begins_at': str(dendra_fetch.iloc[index]['timestamp_utc']).replace(\" \",\"T\").replace(\"+00:00\",\".000Z\"),\n",
    "#             'ends_before': str(dendra_fetch.iloc[index]['timestamp_utc']).replace(\" \",\"T\").replace(\"+00:00\",\".000Z\")\n",
    "#         } for index in outlier_indices]\n",
    "        \n",
    "#         #Create annotation object\n",
    "#         outlier_anno['datastream_ids'] = [datastream_id]\n",
    "#         outlier_anno['intervals'] = time_stamps\n",
    "#         outlier_anno['station_ids'] = [station_id]\n",
    "#         outlier_anno['title'] = f\"Outlier Filter for {station_id}:{datastream['name']}\"\n",
    "#         outlier_anno['description'] = \"Isolation Forest Outlier Filter + Removing Negative Points\"\n",
    "\n",
    "#         print(f\"{station_name}:{datastream['name']}:{len(time_stamps)} outliers found\")\n",
    "# #         Write the JSON objects to a file\n",
    "#         with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Annotations/{datastream_id}.outliers.annotation.json\", 'w') as json_file:\n",
    "#             json.dump(outlier_anno, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87fbd4",
   "metadata": {},
   "source": [
    "# Create Derived Datastreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Iterate through stations\n",
    "#     #Find level datastreams\n",
    "#     #Find temperature datastreams\n",
    "   \n",
    "# with open(f\"C:\\\\Users\\\\jinsu.elhance\\\\Box\\\\000. Jinsu Elhance\\\\DendraWork\\\\Datastreams\\\\Derived\\\\level.template.json\", encoding=\"utf-8\") as level_template_file:\n",
    "    \n",
    "#     level_template_json = json.load(level_template_file)\n",
    "    \n",
    "# level_template_json = deepcopy(level_template_json)\n",
    "\n",
    "# with open(f\"C:\\\\Users\\\\jinsu.elhance\\\\Box\\\\000. Jinsu Elhance\\\\DendraWork\\\\Datastreams\\\\Derived\\\\temp.template.json\", encoding=\"utf-8\") as temp_template_file:\n",
    "    \n",
    "#     temp_template_json = json.load(temp_template_file)\n",
    "    \n",
    "# temp_template_json = deepcopy(temp_template_json)\n",
    "\n",
    "# for station_name in stations_dict:\n",
    "#     station_id = stations_dict[station_name]\n",
    "#     station_datastreams = dendra.list_datastreams_by_station_id(station_id)\n",
    "#     station_name_C = dendra.get_meta_station_by_id(station_id)['full_name']\n",
    "        \n",
    "#     level_xle, level_xlsx, temp_xle, temp_xlsx = None, None, None, None\n",
    "#     level_json = deepcopy(level_template_json)\n",
    "#     temp_json = deepcopy(temp_template_json)\n",
    "#     to_hide = True\n",
    "    \n",
    "#     for datastream in station_datastreams: \n",
    "#         datastream_id = datastream['_id']\n",
    "        \n",
    "#         if datastream['name'] == \"Well Water Level xle/lev\":\n",
    "#             level_xle = datastream\n",
    "#         elif datastream['name'] == \"Well Water Level xlsx\":\n",
    "#             level_xlsx = datastream\n",
    "#         elif datastream['name'] == \"Well Water Temperature xle/lev\":\n",
    "#             temp_xle = datastream\n",
    "#         elif datastream['name'] == \"Well Water Temperature xlsx\":\n",
    "#             temp_xlsx = datastream\n",
    "\n",
    "#     #Create Derived Level datastream\n",
    "#     level_json['description'] = f\"Derived datastream for {station_name_C} Well Water Level\"\n",
    "#     level_json['derived_from_datastream_ids'] = [level_xle['_id'], level_xlsx['_id']]\n",
    "#     level_json['name'] = \"Well Water Level\"\n",
    "#     level_json['station_id'] = station_id\n",
    "#     level_json['is_hidden'] = dendra.get_meta_datastream_by_id(level_xle['_id'])['is_hidden'] and dendra.get_meta_datastream_by_id(level_xlsx['_id'])['is_hidden']\n",
    "    \n",
    "#     with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Derived/{station_name}.level.derived.json\", 'w') as json_file:\n",
    "#         json.dump(level_json, json_file, indent=4)\n",
    "    \n",
    "#     #Create Derived temperature datastream\n",
    "#     temp_json['description'] = f\"Derived datastream for {station_name_C} Well Water Temperature\"\n",
    "#     temp_json['derived_from_datastream_ids'] = [temp_xle['_id'], temp_xlsx['_id']]\n",
    "#     temp_json['name'] = \"Well Water Temperature\"\n",
    "#     temp_json['station_id'] = station_id\n",
    "#     temp_json['is_hidden'] = dendra.get_meta_datastream_by_id(temp_xle['_id'])['is_hidden'] and dendra.get_meta_datastream_by_id(temp_xlsx['_id'])['is_hidden']\n",
    "    \n",
    "#     with open(f\"C:/Users/jinsu.elhance/Box/000. Jinsu Elhance/DendraWork/Datastreams/Derived/{station_name}.temp.derived.json\", 'w') as json_file:\n",
    "#         json.dump(temp_json, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee00ec2",
   "metadata": {},
   "source": [
    "# Barometrically Correct (if possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b487a5ec",
   "metadata": {},
   "source": [
    "# Use Survey123 Data to Create Station Attributes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
