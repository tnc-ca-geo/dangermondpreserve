{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9094f955",
   "metadata": {},
   "source": [
    "### Pulling in LEV files and to Construct Deployment, Well, and Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb13921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T18:06:23.149256Z",
     "start_time": "2023-02-15T18:06:13.958121Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import os, glob, linecache, uuid\n",
    "import xml.etree.ElementTree as ETE\n",
    "import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from scipy.stats import zscore\n",
    "import re\n",
    "import ipdb\n",
    "\n",
    "uuid_gen = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1a7d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T18:06:23.197139Z",
     "start_time": "2023-02-15T18:06:23.183140Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/jinsu.elhance/Box/Wells/WellsDatasheets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61d8a2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T18:06:24.505122Z",
     "start_time": "2023-02-15T18:06:23.205134Z"
    }
   },
   "outputs": [],
   "source": [
    "LEV_dir_list = glob.glob(f\"{data_dir}/*/*.lev\")\n",
    "XLE_dir_list = glob.glob(f\"{data_dir}/*/*.xle\") + glob.glob(f\"{data_dir}/*/*/*.xle\")\n",
    "XLSX_dir_list = glob.glob(f\"{data_dir}/WellsData_Dudek_2009-2011/*.xlsx\")\n",
    "CSV_dir_2017_2018_list = glob.glob(f\"{data_dir}/WellsData_2017-2018/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a926f072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T18:06:25.372667Z",
     "start_time": "2023-02-15T18:06:24.542675Z"
    }
   },
   "outputs": [],
   "source": [
    "processed = set([os.path.basename(file) for file in LEV_dir_list + XLE_dir_list + XLSX_dir_list + CSV_dir_2017_2018_list])\n",
    "unprocessed_list = [glob.glob(f\"{data_dir}/*/*.{ext}\") for ext in [\"xlsx\", \"lev\", \"xle\", \"csv\"]] + [glob.glob(f\"{data_dir}/*/*/*.xle\")]\n",
    "unprocessed = set()\n",
    "for ext in unprocessed_list:\n",
    "    unprocessed.update([os.path.basename(file) for file in ext])\n",
    "\n",
    "unprocessed = unprocessed.difference(processed)\n",
    "\n",
    "with open(f\"{data_dir}/SynthesisStatus.txt\", \"w\") as output:\n",
    "    output.write(\"Unprocessed datasheets \\n\")\n",
    "    for file in unprocessed:\n",
    "        output.write(str(file) + \"\\n\")\n",
    "    output.write(\"Emails\\n\")\n",
    "    output.write(\"\\nProcessed datasheets \\n\")\n",
    "    for file in processed:\n",
    "        output.write(str(file) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99fd53e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T00:27:12.711066Z",
     "start_time": "2022-12-06T00:27:12.693651Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_data_rows(lines):\n",
    "    i = 0 \n",
    "    while i < len(lines):\n",
    "        if lines[i] == \"[Data]\\r\\n\":\n",
    "            return i \n",
    "        i += 1\n",
    "        \n",
    "def getXLEMetadata(xle_dict) :\n",
    "    _metadata = {}\n",
    "    _instrumentInfo = xle_dict.get(\"Instrument_info\")\n",
    "    _instrumentDataInfo = xle_dict.get(\"Instrument_info_data_header\")\n",
    "    \n",
    "    _metadata['Instrumenttype'] = _instrumentInfo.get('Instrument_type')\n",
    "    _metadata['Instrumentstate'] = _instrumentInfo.get('Instrument_state')\n",
    "    _metadata['Serialnumber'] = _instrumentInfo.get(\"Serial_number\")\n",
    "    _metadata['Location'] = (_instrumentDataInfo.get('Location') or \"Unknown\").replace(\"/\", \"\").replace(\"#\", \"\").replace(\" \",\"\").lower()\n",
    "    _metadata['SampleRate'] = _instrumentDataInfo.get(\"Sample_rate\")\n",
    "    _metadata['SampleMode'] = _instrumentDataInfo.get(\"Sample_mode\")\n",
    "    _metadata['Altitude'] = xle_dict.get('Ch1_data_header', {}).get('Parameters', {}).get('Altitude', 0)\n",
    "    _metadata['StartTime'] = _instrumentDataInfo.get('Start_time') or 'Unknown'\n",
    "    _metadata['StopTime'] = _instrumentDataInfo.get('Stop_time') or 'Unknown'\n",
    "    _metadata['LevelUnit'] = xle_dict.get('Ch1_data_header', {}).get('Unit', 'unit')\n",
    "    _metadata['TemperatureUnit'] = xle_dict.get('Ch2_data_header', {}).get('Unit', 'unit')[-1]\n",
    "    \n",
    "    return _metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a1a801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WellDevices = pd.DataFrame()\n",
    "WellData = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4532ed5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopy2(lev, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/HistoricalWellSynthesis/Data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lev)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m## Find Data Pointer\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m _df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_fwf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dataStart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlevel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miso-8859-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m _df \u001b[38;5;241m=\u001b[39m _df\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     32\u001b[0m _df \u001b[38;5;241m=\u001b[39m _df\u001b[38;5;241m.\u001b[39mset_index(_df\u001b[38;5;241m.\u001b[39magg((\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0[date]}\u001b[39;00m\u001b[38;5;132;01m{0[time]}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00minstrumentType\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mhash\u001b[39m(x)))\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgeopy\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:762\u001b[0m, in \u001b[0;36mread_fwf\u001b[1;34m(filepath_or_buffer, colspecs, widths, infer_nrows, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer_nrows\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m infer_nrows\n\u001b[0;32m    761\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython-fwf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 762\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgeopy\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:488\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgeopy\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1047\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1046\u001b[0m     nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m-> 1047\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col_dict:\n\u001b[0;32m   1051\u001b[0m             \u001b[38;5;66;03m# Any column is actually fine:\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgeopy\\lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:246\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 246\u001b[0m         content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgeopy\\lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1055\u001b[0m, in \u001b[0;36mPythonParser._get_lines\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m   1052\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m     new_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_iter_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m     rows \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgeopy\\lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:742\u001b[0m, in \u001b[0;36mPythonParser._next_iter_line\u001b[1;34m(self, row_num)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;66;03m# assert for mypy, data is Iterator[str] or None, would error in next\u001b[39;00m\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m csv\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    745\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_bad_lines \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBadLineHandleMethod\u001b[38;5;241m.\u001b[39mERROR\n\u001b[0;32m    746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_bad_lines \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBadLineHandleMethod\u001b[38;5;241m.\u001b[39mWARN\n\u001b[0;32m    747\u001b[0m     ):\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgeopy\\lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1190\u001b[0m, in \u001b[0;36mFixedWidthReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1188\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf)\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# Note: 'colspecs' is a sequence of half-open intervals.\u001b[39;00m\n\u001b[1;32m-> 1190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [line[fromm:to]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter) \u001b[38;5;28;01mfor\u001b[39;00m (fromm, to) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolspecs]\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgeopy\\lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1190\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1188\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf)\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# Note: 'colspecs' is a sequence of half-open intervals.\u001b[39;00m\n\u001b[1;32m-> 1190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [line[fromm:to]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter) \u001b[38;5;28;01mfor\u001b[39;00m (fromm, to) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolspecs]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lev in LEV_dir_list:\n",
    "    with open(lev, newline=\"\\n\") as lev_text:\n",
    "        lev_lines = lev_text.readlines()\n",
    "        _dataStart = find_data_rows(lev_lines) + 2\n",
    "        \n",
    "        #Pull out metadata from datafile\n",
    "        _metadata = {}\n",
    "        for _mdx in lev_lines[10:_dataStart]:\n",
    "            _spltKeyDat = _mdx.replace(\" \", \"\").strip().split(\"=\")\n",
    "            if len(_spltKeyDat) > 1:\n",
    "                if _spltKeyDat[0] == \"Unit\" and \"LevelUnit\" in _metadata.keys():\n",
    "                    _spltKeyDat[0] = \"TemperatureUnit\"\n",
    "                elif _spltKeyDat[0] == \"Unit\":\n",
    "                    _spltKeyDat[0] = \"LevelUnit\"\n",
    "                _metadata[_spltKeyDat[0]] = _spltKeyDat[1]\n",
    "\n",
    "        #Which well?\n",
    "        location = (_metadata.get(\"Location\") or \"Unknown\").replace(\"/\", \"\").replace(\"#\", \"\").replace(\" \",\"\").lower()\n",
    "        instrumentType = _metadata.get('Instrumenttype' or \"Unknown\")\n",
    "        \n",
    "        #Create folder for storing data\n",
    "        if not os.path.exists(f\"{data_dir}/HistoricalWellSynthesis/Data/{location}\"):\n",
    "            os.makedirs(f\"{data_dir}/HistoricalWellSynthesis/Data/{location}\")\n",
    "            \n",
    "        #Copy datasheet to data store directory\n",
    "        if not os.path.exists(f\"{data_dir}/HistoricalWellSynthesis/Data/{location}/{os.path.basename(lev)}\"):\n",
    "            shutil.copy2(lev, f\"{data_dir}/HistoricalWellSynthesis/Data/{location}/{os.path.basename(lev)}\")\n",
    "            \n",
    "        ## Find Data Pointer\n",
    "        _df = pd.read_fwf(lev, skiprows=_dataStart, names=[\"date\", \"time\", \"level\", \"temperature\"], encoding='iso-8859-1')\n",
    "        _df = _df.iloc[:-1]\n",
    "        _df = _df.set_index(_df.agg(('{0[date]}{0[time]}' + f\"{location}{instrumentType}\").format, axis=1).apply(lambda x: hash(x)))\n",
    "        _df['date'] = pd.to_datetime(_df['date'])\n",
    "        \n",
    "        #Any data after 2020 will be barometrically corrected.\n",
    "        if \"2020\" in lev or \"2021\" in lev:\n",
    "            _df['baro_corrected'] = False\n",
    "        else:\n",
    "            _df['baro_corrected'] = True\n",
    "        \n",
    "        #Update Metadata\n",
    "        _metadata['dataStartDate'] = min(_df['date'])\n",
    "        _metadata['dataEndDate'] = max(_df['date'])\n",
    "        _metadata['TemperatureUnit'] = _metadata['TemperatureUnit'][-1]\n",
    "        _metadata[\"Location\"] = location\n",
    "        \n",
    "        #Apply conversions\n",
    "        if _metadata.get(\"TemperatureUnit\") == \"C\":\n",
    "            _df['temperature_c'] = _df['temperature'].astype(float)\n",
    "            _df['temperature_f'] = _df['temperature'].astype(float) * 1.8 + 32\n",
    "        else:\n",
    "            _df['temperature_f'] = _df['temperature']\n",
    "\n",
    "        if _metadata.get(\"LevelUnit\", \"unit\") == \"m\":\n",
    "            _df['level'] = _df['level'].astype(float) / 3.28084\n",
    "                    \n",
    "        #Save data to dataframes\n",
    "        if location in WellData.keys():\n",
    "            WellData[location] = pd.concat([WellData[location], _df], axis=0)\n",
    "        else:\n",
    "            WellData[location] = _df\n",
    "\n",
    "        WellDevices = pd.concat([WellDevices, pd.DataFrame(_metadata, index=[instrumentType+location])], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "904538e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml_to_dict\n",
    "\n",
    "parser = xml_to_dict.XMLtoDict()\n",
    "\n",
    "#Similar data-read to the LEV read. However, makes use of a metadata parsing helper.\n",
    "for xle in XLE_dir_list:\n",
    "    \n",
    "    with open(xle, newline=\"\\n\") as xle_text:\n",
    "        xle_content = xle_text.read()\n",
    "        xle_dict = parser.parse(xle_content)['Body_xle']\n",
    "        \n",
    "        _metadata = getXLEMetadata(xle_dict)\n",
    "        location = _metadata['Location']\n",
    "        \n",
    "        if not os.path.exists(f\"{data_dir}/HistoricalWellSynthesis/Data/{location}\"):\n",
    "            os.makedirs(f\"{data_dir}/HistoricalWellSynthesis/Data/{location}\")\n",
    "            \n",
    "        if not os.path.exists(f\"{data_dir}/HistoricalWellSynthesis/Data/{location}/{os.path.basename(xle)}\"):\n",
    "            shutil.copy2(xle, f\"{data_dir}/HistoricalWellSynthesis/Data/{location}/{os.path.basename(xle)}\")\n",
    "            \n",
    "        _df = pd.DataFrame(xle_dict.get('Data', {}).get('Log', {}))\n",
    "        _df = _df.set_index(_df.agg(('{0[Date]}{0[Time]}' + f\"{location}{instrumentType}\").format, axis=1).apply(lambda x: hash(x)))\n",
    "        _df = _df.rename(columns = {\"Date\": 'date', \"Time\":'time', \"ch1\":\"level\", 'ch2':'temperature' }).drop(['ms', '@id'], axis=1)\n",
    "        \n",
    "        if _metadata['TemperatureUnit'] == \"C\":\n",
    "            _df['temperature_c'] = _df['temperature'].astype(float)\n",
    "            _df['temperature_f'] = _df['temperature'].astype(float) * 1.8 + 32\n",
    "        else:\n",
    "            _df['temperature_f'] = _df['temperature']\n",
    "\n",
    "        if _metadata['LevelUnit'] ==\"m\":\n",
    "            _df['level'] = _df['level'].astype(float) / 3.28084\n",
    "            \n",
    "        _metadata['dataStartDate'] = min(_df['date'])\n",
    "        _metadata['dataEndDate'] = max(_df['date'])\n",
    "        \n",
    "        #Any data after 2020 will need to be barometrically corrected.\n",
    "        if \"2020\" in xle or \"2021\" in xle:\n",
    "            _df['baro_corrected'] = False\n",
    "        else:\n",
    "            _df['baro_corrected'] = True\n",
    "        \n",
    "        if location in WellData.keys():\n",
    "            WellData[location] = pd.concat([WellData[location], _df], axis=0)\n",
    "        else: \n",
    "            WellData[location] = _df\n",
    "            \n",
    "        WellDevices = pd.concat([WellDevices, pd.DataFrame(_metadata, index=[instrumentType+location])], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a67b7dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T00:30:48.862167Z",
     "start_time": "2022-12-06T00:27:32.910844Z"
    }
   },
   "outputs": [],
   "source": [
    "for xlsx in XLSX_dir_list:\n",
    "    location = xlsx.split(\"\\\\\")[1][:-5].replace(\"_\",\"\").replace(\" \",\"\").lower()\n",
    "\n",
    "    if not os.path.exists(f\"{data_dir}/HistoricalWellSynthesis/Data/{location}\"):\n",
    "        os.makedirs(f\"{data_dir}/HistoricalWellSynthesis/Data/{location}\")\n",
    "    \n",
    "    if not os.path.exists(f\"{data_dir}/HistoricalWellSynthesis/Data/{location}/{os.path.basename(xlsx)}\"):\n",
    "        shutil.copy2(xlsx, f\"{data_dir}/HistoricalWellSynthesis/Data/{location}/{os.path.basename(xlsx)}\")\n",
    "        \n",
    "    _df = pd.read_excel(xlsx, sheet_name = \"data\")\n",
    "    _df = _df[[\"DATE TIME\", \"Level (ft)\", \"Temperature °C\", \"DATE TIME.1\", \"Barologger Level (ft)\", \"Barologger Corrected for Elevation\", \"Temperature °C.1\", \"Corrected Water Level (ft H2O)\", \"Groundwater Elevation (ft MSL)\"]]\n",
    "\n",
    "    _df.loc[_df['DATE TIME.1'].isna(), 'DATE TIME.1'] = _df.loc[_df['DATE TIME.1'].isna(), 'DATE TIME']\n",
    "    _df.loc[_df['DATE TIME'].isna(), 'DATE TIME'] = _df.loc[_df['DATE TIME'].isna(), 'DATE TIME.1']\n",
    "\n",
    "    _df = _df.loc[_df['Temperature °C'].apply(type) == float] \n",
    "\n",
    "    _df['date'] = _df['DATE TIME'].dt.date\n",
    "    _df['time'] = _df['DATE TIME'].dt.time\n",
    "    _df['temperature_f'] = _df['Temperature °C'].astype(float)  * 1.8 + 32\n",
    "    _df['temperature_barometer_f'] = _df['Temperature °C.1'].astype(float) * 1.8 + 32\n",
    "    _df['baro_corrected'] = True\n",
    "    \n",
    "    _df = _df.rename(columns = {\n",
    "        'Level (ft)': 'level', \n",
    "        'temperature_f':'temperature_f', \n",
    "        'Temperature °C': 'temperature_c',\n",
    "        'Barologger Level (ft)':'barologger_level', \n",
    "        'Barologger Corrected for Elevation': 'barologger_level_c_elevation',\n",
    "        'Temperature °C.1': 'temperature_barometer_c',\n",
    "        'Corrected Water Level (ft H2O)': 'level_corrected',\n",
    "        'Groundwater Elevation (ft MSL)': 'groundwater_elevation'}\n",
    "        )[['date','time','level','temperature_f','barologger_level','barologger_level_c_elevation','temperature_barometer_f','level_corrected','groundwater_elevation']]\n",
    "\n",
    "    #Some barologgers collect at longer intervals, forward fill values.\n",
    "    _df['barologger_level'] = _df['barologger_level'].ffill(axis=0)\n",
    "    _df['barologger_level_c_elevation'] = _df['barologger_level_c_elevation'].ffill(axis=0) #barologger level, corrected for elevation.\n",
    "    _df['temperature_barometer_f']  = _df['temperature_barometer_f'].ffill(axis=0)\n",
    "    _df['level_corrected'] = _df['level'] - _df['barologger_level_c_elevation']\n",
    "    \n",
    "    transducer_depth_f = _df['groundwater_elevation'].values[0] - _df['level_corrected'].values[0]\n",
    "    _df['groundwater_elevation'] = _df['level_corrected'] + transducer_depth_f\n",
    "    \n",
    "    if location in WellData.keys():\n",
    "        WellData[location] = pd.concat([WellData[location], _df], axis=0)\n",
    "    else:\n",
    "        WellData[location] = _df\n",
    "        \n",
    "#Not updating devices from xlsxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04fbc2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgeopy\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3460: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# WellDevices = pd.read_csv(f\"{data_dir}/HistoricalWellSynthesis/Data/WellDevices.csv\")\n",
    "# full_well_data = pd.read_csv(f\"{data_dir}/HistoricalWellSynthesis/Data/full_well_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32aa9821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-c594f08cb2d9>:2: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  Devices_2020['Well_Name_std'] = Devices_2020['Well_Name'].str.replace(\"/\", \"\").str.replace(\"#\", \"\").str.replace(\" \",\"\").str.replace(\".\", \"\").str.lower()\n"
     ]
    }
   ],
   "source": [
    "#Find well elevations from Devices 2020 csv.\n",
    "Devices_2020 = pd.read_csv(\"C:/Users/jinsu.elhance/Box/Wells/JLDP_Wells_Sensor_List_2020_08_19.csv\")\n",
    "Devices_2020['Well_Name_std'] = Devices_2020['Well_Name'].str.replace(\"/\", \"\").str.replace(\"#\", \"\").str.replace(\" \",\"\").str.replace(\".\", \"\").str.lower()\n",
    "Devices_2020['Well_Name_std']\n",
    "well_elevations_ft = (Devices_2020.groupby(\"Well_Name_std\")['Elevation_m'].first() * 3.28084).to_dict()\n",
    "\n",
    "#Add elevations for missing wells.\n",
    "well_elevations_ft['tinta3bl'] = well_elevations_ft['tinta3']\n",
    "well_elevations_ft['lowerjalamavaqueros'] = 492.16\n",
    "well_elevations_ft['quailcanyon1'] = well_elevations_ft['quailcanyon']\n",
    "well_elevations_ft['tinta11b'] = well_elevations_ft['tinta11a']\n",
    "well_elevations_ft['tinta5b'] = well_elevations_ft['tinta5']\n",
    "well_elevations_ft['oaks3bbl'] = well_elevations_ft['oaks3']\n",
    "well_elevations_ft['venidido2'] = well_elevations_ft['venadito2'] #Is venidido a different well?\n",
    "well_elevations_ft['escondido3a'] = well_elevations_ft['escondido3']\n",
    "well_elevations_ft['unknown'] = 0\n",
    "well_elevations_ft['oaks3a'] = well_elevations_ft['oaks3']\n",
    "\n",
    "#The elevation of K7AZ Lompoc MesoWest Weather Station\n",
    "baro_elevation_ft = 1529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e280ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WellDevices = WellDevices.set_index(WellDevices[\"Instrumenttype\"] + WellDevices[\"Location\"])\n",
    "WellDevices['Altitude'] = WellDevices['Altitude'].astype(str).apply(lambda x: x.replace(\"ft\", \"\"))\n",
    "WellDevices = WellDevices.drop_duplicates()\n",
    "WellDevices.groupby([\"Location\", \"dataStartDate\"]).first()\n",
    "\n",
    "WellDevices = WellDevices[[\n",
    "    \"Instrumenttype\",\n",
    "    \"Serialnumber\",\n",
    "    \"Location\",\n",
    "    \"SampleRate\",\n",
    "    \"Altitude\",\n",
    "    \"StartTime\",\n",
    "    \"StopTime\",\n",
    "    \"LevelUnit\",\n",
    "    \"TemperatureUnit\",\n",
    "    \"dataStartDate\",\n",
    "    \"dataEndDate\",\n",
    "]]\n",
    "\n",
    "WellDevices.to_csv(f\"{data_dir}/HistoricalWellSynthesis/Data/WellDevices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d901af2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T00:31:14.137722Z",
     "start_time": "2022-12-06T00:30:49.113670Z"
    }
   },
   "outputs": [],
   "source": [
    "#construct full well data\n",
    "full_well_data = pd.DataFrame()\n",
    "for Well, Data in WellData.items():\n",
    "    Data[\"well\"] = Well\n",
    "    full_well_data = pd.concat([full_well_data, Data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0dfabb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure all entries have an associated timestamp, otherwise remove.\n",
    "full_well_data = full_well_data.loc[~full_well_data['time'].isna()]\n",
    "\n",
    "#Convert datetime column to a datetime data type\n",
    "full_well_data['datetime'] = pd.to_datetime(full_well_data['date'].astype(str) + \" \" + full_well_data['time'].astype(str));\n",
    "\n",
    "#Change column types to numeric\n",
    "full_well_data['level'] = full_well_data['level'].astype(float)\n",
    "full_well_data['temperature_f'] = full_well_data['temperature_f'].astype(float)\n",
    "full_well_data['temperature_c']= full_well_data['temperature_c'].astype(float)\n",
    "\n",
    "#For any missing level values, take corrected level. Some CSVs only contained \"corrected levels\"\n",
    "full_well_data.loc[full_well_data['level'].isna(), 'level'] = full_well_data.loc[full_well_data['level'].isna()]['level_corrected']\n",
    "full_well_data = full_well_data.drop_duplicates()\n",
    "full_well_data['datetime_hr_10min'] = full_well_data.datetime.astype(str).apply(lambda x: x[:-4])\n",
    "\n",
    "# full_well_data.to_csv(f\"{data_dir}/HistoricalWellSynthesis/Data/full_well_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
